{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "We're going to briefly look at truth tables. As an example let's assume we have a 100 photos, some of birds and some of people. In a truth table you assume you can categorize the true answer‚Äîlet's say you pay an undergraduate to sort the photos into birds and people. Then you want to have an automated sorting algorithm. However that works, whether it is by looking a the color at the center of photo or using a neural network (NN), it will also sort the photos but imperfectly. Given two sorts you can then arrange a truth table:\n",
    "\n",
    "|           | True Bird | True Person |\n",
    "|-----------|-----------|-------------|\n",
    "| NN Bird   | 45        | 5           |\n",
    "| NN Person | 3         | 47          |\n",
    "\n",
    "There are 48 birds and 52 People (columns), and there are 8 miscategorizations. The NN called three of the birds people, and 5 of the people birds.\n",
    "\n",
    "This is useful because it not only shows the number of errors, but the type, and not all errors are created equal. Let's say our identification system is being used to keep birds from escaping the aviary at the zoo by locking the aviary door when a bird is trying to escape. We really don't want to let birds escape, but locking a person in for 30 extra seconds is not a big deal. In this case false bird identification is not so bad, but false person identification lets a bird escape.\n",
    "\n",
    "Let's say the above truth table is the current system, and you've developed a new algorithm (NA) with the following truth table:\n",
    "\n",
    "|           | True Bird | True Person |\n",
    "|-----------|-----------|-------------|\n",
    "| NA Bird   | 47        | 11          |\n",
    "| NA Person | 1         | 41          |\n",
    "\n",
    "\n",
    "#### 1a) Which algorithm makes the fewest mistakes?\n",
    "*Answer* : NN makes fewer mistakes, if we are just summing them. NN made 8 total mistakes, while NA made 12.\n",
    "\n",
    "#### 1b) Which algorithm is better for the zoo? Explain.\n",
    "*Answer*: NA is better, because less of its mistakes miscategorized birds as people, meaning less birds would get away. The people can be held up at a lesser cost than the birds flying away.\n",
    "\n",
    "#### 1c) During the pandemic the number of visitors plummets, and it is only the zoo keeper visiting. So instead of 52% of the photos taken at the aviary door being people, it is now only 1%. Make new truth tables for both algorithms.\n",
    "\n",
    "*Answer*:\n",
    "\n",
    "|           | True Bird | True Person |\n",
    "|-----------|-----------|-------------|\n",
    "| NA Bird   |           |             |\n",
    "| NA Person |           |             |\n",
    "\n",
    "\n",
    "|           | True Bird | True Person |\n",
    "|-----------|-----------|-------------|\n",
    "| NA Bird   | 47        | 11          |\n",
    "| NA Person | 1         | 42          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9038461538461539 0.9791666666666666 0.057692307692307696 0.10416666666666667\n",
      "47.0 5.0 0.9038461538461539 0.057692307692307696\n"
     ]
    }
   ],
   "source": [
    "# initial stats for NN\n",
    "bird = (48/100) * 100\n",
    "person = (52/100) * 100\n",
    "acc_person = 47/person\n",
    "acc_bird = 47/bird\n",
    "fake_person = 3/person\n",
    "fake_bird = 5/bird\n",
    "print(acc_person, acc_bird, fake_person, fake_bird)\n",
    "\n",
    "new_person = (1/100) * 100\n",
    "new_bird = (48/100) * 100\n",
    "new_acc_bird = new_bird * acc_bird\n",
    "new_fake_bird = new_bird * fake_bird\n",
    "new_acc_person = new_person * acc_person\n",
    "new_fake_person = new_person * fake_person\n",
    "print(new_acc_bird, new_fake_bird, new_acc_person, new_fake_person)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "In the last lab we explored how to numerically calculate the pdf of a summed or averaged observation through repeated convolutions. But sometimes the convolution has an analytical solution. We could have found this out by either using a sharp pencil and doing the convolution integral by hand, or by looking it up in a table (much easier).\n",
    "\n",
    "Having an analytic answer is much nicer when they exist, so it is always good to look and see if it exists. Further, sums and averages are only some of the mathematical operations that we can perform. In this section we will do an internet scavenger hunt to find the analytic pdf for some interesting distributions.\n",
    "\n",
    "### Example 1\n",
    "What is the sum of two Guassian distributions?\n",
    "\n",
    "We did this numerically in the last lab, but we can find it analytically. One might start with this page on the normal distribution which would refer you to this page on the sum, which would give you the same answer you figured out last week.\n",
    "\n",
    "### Example 2\n",
    "Let's say we have a variable with a Rayleigh distribution, and we're going to square it. What is the distribution?\n",
    "\n",
    "First I'll lookup and read about the Rayleigh distribution, such as this Wikipedia page (Mathworld and other sources, such as CRC books are great too). Down near the bottom are listed a number of related distributions. Note that the square of the Rayleigh is listed as a gamma distribution with N = 1. Looking up the gamma distribution we see that a gamma with N=1 is an exponential distribution, and just to check we can see that the sqrt of an exponential distribution is a Rayleigh distribution to bring us full circle.\n",
    "\n",
    "Now it is your turn!\n",
    "\n",
    "#### 2a) What is the pdf of the sum of two identical exponential distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b) What is the pdf of the ratio of two zero-mean unity variance normal distributions  ùëã1/ùëã2 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2c) So far we have looked at 1D probability distributions, but it is possible to have a multi-dimensional vector distribution. A simple first introduction is the 2D Guassian; it looks like a smeared spot. Mathematically this is given by  ùëãùëñÃÇ +ùëåùëóÃÇ   where both  ùëã  and  ùëå  are drawn from 1D Gaussian distributions. If I measure the amplitude of this vector, what is its pdf? (Hint, the amplitude is always positive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
